#!/bin/bash
#
# ---------------------------------------- #
#   Daniel Smith - February 15th, 2022     #
#   Baylor College of Medicine             #
#   Creative Commons License BY-SA 4.0     #
# ---------------------------------------- #
#


#----------------------------------------------------------
# Default options for the script's parameters.
#----------------------------------------------------------
DEST=""; INFILE=""; TEST=0; CONFIG=""; EXT="";
STORE="INTELLIGENT_TIERING"; ROTATE="5/7/6/12/5"; 


#----------------------------------------------------------
# Help text
#----------------------------------------------------------
usage () {
cat <<-EOM

  THIS SCRIPT WILL DELETE FILES. THE AUTHOR MAKES NO GUARANTEE 
  IT WILL ONLY DELETE THE RIGHT ONES. USE AT YOUR OWN RISK.
  
  Usage: $0 -d s3://bucket/path/prefix_ [OPTIONS]
  
      -d --dest    Destination path for backups.
      -r --rotate  Rotation scheme in recent/day/week/month/year format.
                   [Default: $ROTATE]
  
      -a --add     A file or folder to push to the backup destination.
      -e --ext     Append this file extension to uploaded files.
                   [Default: file=autodetect; folder=.tar.gz]
  
      -t --test    Do not change files - dry run only.
      -s --store   S3 storage class to use. [Default: $STORE]
      -c --config  Path to the AWS config file to use.
      -h --help    Print this help message and exit.
  
  
  Removes old versions of backups according to the user's data retention
  policy (keep X daily, Y weekly, Z monthly, etc).
  
  Example:
    $0 --dest s3://xyz-backup/x_ --rotate /3/6 --test
  
    s3://xyz-backup/x_2022-01-01-0500.tgz    KEEP
    s3://xyz-backup/x_2022-01-02-0500.tgz    DROP
    s3://xyz-backup/x_2022-01-06-0500.tgz    DROP
    s3://xyz-backup/x_2022-01-07-0500.tgz    KEEP
    s3://xyz-backup/x_2022-01-08-0500.tgz    KEEP
    s3://xyz-backup/x_2022-01-09-0500.tgz    KEEP
  
  This script relies on datestamps in the file's name. It completely ignores any
  creation/modification file metadata. The current date also does not factor
  into which files are kept or discarded; it keeps the most recent N files from
  each time category regardless of how old they are. Autogenerated timestamp is
  the current timezone's date/time in YYYY-mm-dd-HHMM format.
  
EOM
}


#----------------------------------------------------------
# Error text
#----------------------------------------------------------
errmsg () {
	>&2 echo "Error: $1"
}



#--------------------------------------------------------------------
# Parse command line options into variables.
#--------------------------------------------------------------------
[ $# -ne 0 ] || { usage; exit 0; }
while [ $# -gt 0 ]; do
  case "$1" in
    -d|--dest)    DEST="$2";            shift 2 ;;
    -r|--rotate)  ROTATE="$2";          shift 2 ;;
    -a|--add)     INFILE="$2";          shift 2 ;;
    -e|--ext)     EXT="$2";             shift 2 ;;
    -c|--config)  CONFIG="$2";          shift 2 ;;
    -s|--store)   STORE="$2";           shift 2 ;;
    -t|--test)    TEST=1;               shift 1 ;;
    -h|--help)    usage;                exit 0  ;;
    *) errmsg "Invalid Parameter '$1'"; exit 1
  esac
done


#--------------------------------------------------------------------
# Sanity check arguments.
#--------------------------------------------------------------------
[ -n "$DEST" ]                   || { errmsg "--dest is required.";                  exit 1; }
[ -z "$INFILE" -o -r "$INFILE" ] || { errmsg "Can't read --add file '$INFILE'.";     exit 1; }
[ -z "$CONFIG" -o -r "$CONFIG" ] || { errmsg "Can't read --config file '$CONFIG'.";  exit 1; }
[ "${DEST:0:5}" == "s3://" ]     || { errmsg "-o must start with 's3://'.";          exit 1; }

IFS=/ read RECENT DAILY WEEKLY MONTHLY YEARLY <<< "$ROTATE"
case ${RECENT:=0}  in *[!0-9]*) errmsg "recent must be an integer.";   exit 1 ;; esac
case ${DAILY:=0}   in *[!0-9]*) errmsg "daily must be an integer.";    exit 1 ;; esac
case ${WEEKLY:=0}  in *[!0-9]*) errmsg "weekly must be an integer.";   exit 1 ;; esac
case ${MONTHLY:=0} in *[!0-9]*) errmsg "monthly must be an integer.";  exit 1 ;; esac
case ${YEARLY:=0}  in *[!0-9]*) errmsg "yearly must be an integer.";   exit 1 ;; esac

[ "$RECENT$DAILY$WEEKLY$MONTHLY$YEARLY" != "00000" ] || { errmsg "--rotate is required."; exit 1; }

BUCKET=`echo $DEST | cut -f 3 -d '/'`
[ -n "$BUCKET" ] || { errmsg "-o must include a bucket name."; exit 1; }


#--------------------------------------------------------------------
# Check our connection to AWS.
#--------------------------------------------------------------------
[ -z "$CONFIG" ] || export AWS_CONFIG_FILE="$CONFIG"
[ -x "$(command -v aws)" ] || { errmsg "Could not find the 'aws' cli executable."; exit 1; }
aws sts get-caller-identity > /dev/null
[ $? -eq 0 ] || { errmsg "Can't connect to Amazon Web Services."; exit 1; }


#--------------------------------------------------------------------
# Create a directory for our temporary files.
#--------------------------------------------------------------------
T=`mktemp -d`


#--------------------------------------------------------------------
# Upload the new achive to S3, unless in test mode.
#--------------------------------------------------------------------
if [ -n "$INFILE" ]
then
    
  #--------------------------------------------------------------------
  # Autodetect file extension.
  #--------------------------------------------------------------------
  if [ -d "$INFILE" ]
  then
    EXT=".tar.gz"
  elif [ -z "$EXT" ]
  then
    FN=`basename "$INFILE"`
    EXT=`echo "$FN" | rev | sed -r "s/^(.*\.).*/\1/" | rev`
    [ $FN != $EXT ] || EXT=''
  fi
  
  
  if [ $TEST -eq 1 ]
  then
    #--------------------------------------------------------------------
    # Simulate this file being on S3.
    #--------------------------------------------------------------------
    echo "${DEST##*$BUCKET/}$(date '+%F-%H%M')$EXT" > $T/initial
  else
    
    #--------------------------------------------------------------------
    # Compress a directory into a tarball.
    #--------------------------------------------------------------------
    if [ -d "$INFILE" ]
    then
      tar -C `dirname "$INFILE"` -czf "$T/infile$EXT" `basename "$INFILE"`
      INFILE="$T/infile$EXT"
    fi
    
    #--------------------------------------------------------------------
    # Upload to S3, renaming based on prefix and timestamp.
    #--------------------------------------------------------------------
    aws s3 cp "$INFILE" "$DEST$(date '+%F-%H%M')$EXT" --storage-class "$STORE"
    [ $? -eq 0 ] || { errmsg "'aws s3 cp' had non-zero exit status."; exit 1; }
  fi

fi



#--------------------------------------------------------------------
# Fetch the current archive names from S3 and parse out the dates.
#--------------------------------------------------------------------
aws s3 ls --recursive "$DEST" > $T/s3_ls
[ $? -le 1 ] || { errmsg "'aws s3 ls' had non-zero exit status."; exit 1; }

awk '{print $4}' $T/s3_ls >> $T/initial
cat $T/initial                                    |\
  sed -r "s/.*([0-9]{4}-[0-9]{2}-[0-9]{2}).*/\1/" |\
  date '+%F%t%Y-%U%t%Y-%m%t%Y' -f - > $T/extract
paste $T/extract $T/initial | sort -r > $T/search


#--------------------------------------------------------------------
# Now we have the following table, sorted from NEWEST -> OLDEST
#--------------------------------------------------------------------
#|  DAY         WEEK     MONTH    YEAR  FILE
#|  2022-02-09  2022-06  2022-02  2022  x_2022-02-09-1100.tar.gz
#|  2022-02-08  2022-06  2022-02  2022  x_2022-02-08-1000.tar.gz
#|  2022-02-07  2022-06  2022-02  2022  x_2022-02-07-1200.tar.gz
#|  2022-01-31  2022-05  2022-01  2022  x_2022-01-31-0800.tar.gz


#--------------------------------------------------------------------
# Keep the most recent unique N entries from each column.
#--------------------------------------------------------------------
uniq            $T/search | head -n $RECENT  > $T/recent
uniq -w 10      $T/search | head -n $DAILY   > $T/daily
uniq -w 8  -f 1 $T/search | head -n $WEEKLY  > $T/weekly
uniq -w 8  -f 2 $T/search | head -n $MONTHLY > $T/monthly
uniq -w 5  -f 3 $T/search | head -n $YEARLY  > $T/yearly


#--------------------------------------------------------------------
# Intersect the lists to keep, then generate the list to drop.
#--------------------------------------------------------------------
cat $T/recent $T/daily $T/weekly $T/monthly $T/yearly |\
  cut -f 5 | sort -u | grep -v "^$" |\
  sed "s .* s3://$BUCKET/& " > $T/keep

sort -u $T/initial | grep -v "^$" |\
  sed "s .* s3://$BUCKET/& " > $T/initial_sorted
  
comm -23 $T/initial_sorted $T/keep > $T/drop


#--------------------------------------------------------------------
# Remove unneeded archives, or just display our plans to the user.
#--------------------------------------------------------------------
if [ $TEST -eq 1 ]
then
  cat $T/keep | sed "s .* &\tKEEP " >> $T/log
  cat $T/drop | sed "s .* &\tDROP " >> $T/log
  [ -s $T/log ] || echo "(none)"    >> $T/log
  cat $T/log | sort
else
  xargs -a $T/drop -d'\n' -n 1 --no-run-if-empty -I{} \
    sh -c 'aws s3 rm "$1"; [ $? -eq 0 ] || exit 255' sh {} 
fi


#--------------------------------------------------------------------
# Delete our temporary files and directory.
#--------------------------------------------------------------------
rm -rf $T
